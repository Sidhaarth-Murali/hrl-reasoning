defaults:
  - default
  - _self_

# system paths
cache_dir: '/home/pramit/hrl-nips-work/hrl-reasoning/.cache'
save_path: '/home/pramit/hrl-nips-work/hrl-reasoning/.saved_models/score_code'

# checkpoint
checkpoint_path: null

# env settings
env_type: "code"
language: "python"
data_path: "dataset/mbpp_filtered.csv"
env_load_path: null

# model settings
agent_type: "score"
policy_lm: "meta-llama/Llama-3.2-1B-Instruct"
critic_lm: "roberta-base"
max_new_tokens: 512 # Increased for code generation
max_tokens: 512
temperature: 0.7
do_sample: true
use_lora: true
huggingface_token: "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"

# memory optimization settings
use_gradient_checkpointing: true
use_memory_efficient_attention: true
use_8bit_optimizers: true
use_bf16_mixed_precision: true
max_micro_batch_size: 8
gradient_accumulation_steps: 16

# training hyperparameters
capacity: 100000      # Replay buffer size
rollout_size: 32     # Number of parallel environments as specified
eval_size: 8          # Number of trajectories for evaluation
batch_size: 4         # Training batch size
iterations: 280        # Total number of iterations as specified
epochs: 10            # Times to train critic on each batch
actor_epochs: 3       # Times to train actor on each batch
warmup_iter: 6        # Initial iterations without policy updates
grad_accum_steps: 8
critic_lr: 2e-5
lm_lr: 5e-6
stage1_steps: 138      # First stage steps as specified
stage2_steps: 142      # Second stage steps as specified

# SCoRe parameters
alpha: 10.0          # Reward shaping coefficient
beta1: 0.01          # KL coefficient for Stage II
beta2: 0.1           # KL coefficient for Stage I

# algorithm parameters
gamma: 0.95           # Discount factor
tau: 0.1              # Soft update parameter
max_grad_norm: 1.0    # Gradient clipping

save_freq: 23          
eval_freq: 25          

# wandb logging
use_wandb: true
project_name: 'score_code'
run_name: 'score-code-reinforce-kl' 